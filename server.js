import express from "express";
import fetch from "node-fetch";
import cors from "cors";

const app = express();
app.use(cors());
app.use(express.json());

app.get("/", (req, res) => {
  res.send("AI proxy is running! ðŸš€");
});

// ðŸ”¹ API vÃ©gpont a PHP szÃ¡mÃ¡ra
app.get("/api", async (req, res) => {
  const question = req.query.q;

  if (!question) {
    return res.json({ reply: "KÃ©rlek, Ã­rj be egy kÃ©rdÃ©st!" });
  }

  try {
    // Itt hÃ­vhatod az Ollama API-t, OpenAI-t vagy bÃ¡rmi mÃ¡st
    // Most pÃ©ldakÃ©nt az ollama local endpointot hÃ­vjuk:
    const response = await fetch("http://localhost:11434/api/generate", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model: "gemma3:1b",
        prompt: question
      })
    });

    const data = await response.json();
    res.json({ reply: data.response || "Nem talÃ¡ltam vÃ¡laszt a kÃ©rdÃ©sedre." });
  } catch (error) {
    console.error("AI proxy hiba:", error);
    res.json({ reply: "âš ï¸ A szerver nem tudta lekÃ©rni a vÃ¡laszt." });
  }
});

// ðŸ”¹ Port beÃ¡llÃ­tÃ¡sa (Render automatikusan adja)
const PORT = process.env.PORT || 3000;
app.listen(PORT, () => console.log(`âœ… AI proxy fut a ${PORT} porton`));
